{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class Config:\n",
        "    WINDOW_SIZE = 60\n",
        "    N_REGIMES = 3\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 100\n",
        "    FEATURE_DIM = 64\n",
        "    HIDDEN_DIM = 128\n",
        "    LEARNING_RATE = 0.001\n",
        "    TEMPERATURE = 0.1\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    REGIME_NAMES = [\"Bearish\", \"Sideways\", \"Bullish\"]\n",
        "    REGIME_COLORS = [\"#FF5050\", \"#808080\", \"#50C878\"]\n",
        "\n",
        "# 1. Load and preprocess the data with memory optimizations\n",
        "def load_and_preprocess_data(file_path, window_size=60):\n",
        "    print(\"Loading data...\")\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    print(f\"Data loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df.sort_values('date', inplace=True)\n",
        "\n",
        "    tickers = df['ticker'].unique() if 'ticker' in df.columns else ['data']\n",
        "    print(f\"Found {len(tickers)} unique tickers\")\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Processing ticker: {ticker}\")\n",
        "        if 'ticker' in df.columns:\n",
        "            ticker_data = df[df['ticker'] == ticker].copy()\n",
        "        else:\n",
        "            ticker_data = df.copy()\n",
        "\n",
        "        numeric_cols = ticker_data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        if 'date' in numeric_cols:\n",
        "            numeric_cols.remove('date')\n",
        "\n",
        "        if len(ticker_data) < window_size:\n",
        "            print(f\"Not enough data for ticker {ticker}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        ticker_data[numeric_cols] = scaler.fit_transform(ticker_data[numeric_cols])\n",
        "\n",
        "        sequences = []\n",
        "        data_array = ticker_data[numeric_cols].values\n",
        "        for i in range(len(ticker_data) - window_size + 1):\n",
        "            seq = data_array[i:i+window_size]\n",
        "            sequences.append(seq)\n",
        "\n",
        "        if sequences:\n",
        "            all_data[ticker] = np.array(sequences)\n",
        "            print(f\"Created {len(sequences)} sequences of shape {sequences[0].shape}\")\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "    return all_data, tickers\n",
        "\n",
        "# 2. Define the TimeSeriesDataset class with improved augmentations\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, strong_transform=False, weak_transform=False):\n",
        "        self.data = data\n",
        "        self.strong_transform = strong_transform\n",
        "        self.weak_transform = weak_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        if self.strong_transform and self.weak_transform:\n",
        "            strong_aug = self._strong_augment(sample)\n",
        "            weak_aug = self._weak_augment(sample)\n",
        "            return strong_aug, weak_aug\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def _strong_augment(self, x):\n",
        "        \"\"\"Improved strong augmentation with controlled randomness\"\"\"\n",
        "        x_aug = x.copy()\n",
        "\n",
        "        scale = np.random.uniform(0.8, 1.2)\n",
        "        x_aug = x_aug * scale\n",
        "\n",
        "        seq_len, feat_dim = x_aug.shape\n",
        "        num_segments = np.random.randint(3, 6)\n",
        "        segment_size = seq_len // num_segments\n",
        "\n",
        "        segments = []\n",
        "        for i in range(num_segments):\n",
        "            start_idx = i * segment_size\n",
        "            end_idx = (i + 1) * segment_size if i < num_segments - 1 else seq_len\n",
        "            segment = x_aug[start_idx:end_idx].copy()\n",
        "\n",
        "            warp_factor = np.random.uniform(0.9, 1.1)\n",
        "\n",
        "            if warp_factor != 1.0:\n",
        "                new_length = max(3, int(len(segment) * warp_factor))\n",
        "                indices = np.linspace(0, len(segment) - 1, new_length)\n",
        "                warped_segment = np.zeros((new_length, feat_dim))\n",
        "\n",
        "                for j in range(feat_dim):\n",
        "                    warped_segment[:, j] = np.interp(indices, np.arange(len(segment)), segment[:, j])\n",
        "\n",
        "                segments.append(warped_segment)\n",
        "            else:\n",
        "                segments.append(segment)\n",
        "\n",
        "        x_concatenated = np.vstack(segments)\n",
        "        if len(x_concatenated) > seq_len:\n",
        "            x_concatenated = x_concatenated[:seq_len]\n",
        "        elif len(x_concatenated) < seq_len:\n",
        "            padding = np.zeros((seq_len - len(x_concatenated), feat_dim))\n",
        "            x_concatenated = np.vstack([x_concatenated, padding])\n",
        "\n",
        "        return x_concatenated\n",
        "\n",
        "    def _weak_augment(self, x):\n",
        "        \"\"\"Improved weak augmentation for financial time series\"\"\"\n",
        "        x_aug = x.copy()\n",
        "\n",
        "        noise_scale = 0.02 * np.mean(np.abs(x_aug))\n",
        "        noise = np.random.normal(0, noise_scale, x_aug.shape)\n",
        "        x_aug = x_aug + noise\n",
        "\n",
        "        mask_percent = np.random.uniform(0.01, 0.05)\n",
        "        seq_len = x_aug.shape[0]\n",
        "        num_points = int(seq_len * mask_percent)\n",
        "\n",
        "        if num_points > 0:\n",
        "            mask_indices = np.random.choice(np.arange(seq_len), size=num_points, replace=False)\n",
        "            for idx in mask_indices:\n",
        "                if idx > 0 and idx < seq_len - 1:\n",
        "                    x_aug[idx] = (x_aug[idx-1] + x_aug[idx+1]) / 2\n",
        "\n",
        "        return x_aug\n",
        "\n",
        "# 3. Enhanced encoder network with learnable embeddings\n",
        "class TSEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, feature_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(hidden_dim, hidden_dim * 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(hidden_dim * 2, hidden_dim * 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x = self.conv_blocks(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        attn_weights = self.attention(x)\n",
        "        attn_weights = F.softmax(attn_weights, dim=1)\n",
        "\n",
        "        x = torch.sum(x * attn_weights, dim=1)\n",
        "\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 4. Improved contrastive loss with temperature adjustment\n",
        "class NTXentLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.1, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.eps = eps\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        z_i = F.normalize(z_i, dim=1)\n",
        "        z_j = F.normalize(z_j, dim=1)\n",
        "\n",
        "        batch_size = z_i.size(0)\n",
        "        representations = torch.cat([z_i, z_j], dim=0)\n",
        "\n",
        "        similarity_matrix = torch.matmul(representations, representations.t()) / self.temperature\n",
        "\n",
        "        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=similarity_matrix.device)\n",
        "        similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))\n",
        "\n",
        "        labels = torch.cat([\n",
        "            torch.arange(batch_size, 2 * batch_size),\n",
        "            torch.arange(batch_size)\n",
        "        ]).to(similarity_matrix.device)\n",
        "\n",
        "        loss = self.criterion(similarity_matrix, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# 5. Training function with memory optimization and progress tracking\n",
        "def train_ts_tcc(data, tickers, config):\n",
        "    \"\"\"\n",
        "    Train the TS-TCC model with improved memory management\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    embeddings = {}\n",
        "\n",
        "    for ticker in tickers:\n",
        "        ticker_data = data.get(ticker, None)\n",
        "        if ticker_data is None or len(ticker_data) == 0:\n",
        "            print(f\"No data for ticker {ticker}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        input_dim = ticker_data.shape[2]\n",
        "        print(f\"\\nTraining model for {ticker} with input dimension {input_dim}\")\n",
        "\n",
        "        dataset = TimeSeriesDataset(ticker_data, strong_transform=True, weak_transform=True)\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = TSEncoder(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=config.HIDDEN_DIM,\n",
        "            feature_dim=config.FEATURE_DIM\n",
        "        ).to(config.DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=config.LEARNING_RATE,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer,\n",
        "            T_max=config.EPOCHS\n",
        "        )\n",
        "\n",
        "        criterion = NTXentLoss(temperature=config.TEMPERATURE)\n",
        "\n",
        "        progress_bar = tqdm(range(config.EPOCHS), desc=f\"Training {ticker}\")\n",
        "        for epoch in progress_bar:\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch in dataloader:\n",
        "                strong_aug, weak_aug = batch\n",
        "                strong_aug = strong_aug.float().to(config.DEVICE)\n",
        "                weak_aug = weak_aug.float().to(config.DEVICE)\n",
        "\n",
        "                z_i = model(strong_aug)\n",
        "                z_j = model(weak_aug)\n",
        "\n",
        "                loss = criterion(z_i, z_j)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        models[ticker] = model\n",
        "\n",
        "        print(f\"Generating embeddings for {ticker}...\")\n",
        "        model.eval()\n",
        "        dataset_no_aug = TimeSeriesDataset(ticker_data)\n",
        "        dataloader_no_aug = DataLoader(\n",
        "            dataset_no_aug,\n",
        "            batch_size=config.BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        all_embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader_no_aug, desc=\"Extracting embeddings\"):\n",
        "                batch = batch.float().to(config.DEVICE)\n",
        "                embedding = model(batch)\n",
        "                all_embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "                del batch, embedding\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        embeddings[ticker] = np.vstack(all_embeddings)\n",
        "        print(f\"Generated {len(embeddings[ticker])} embeddings for {ticker}\")\n",
        "\n",
        "        del model, dataset, dataloader, dataset_no_aug, dataloader_no_aug\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    return models, embeddings\n",
        "\n",
        "# 6. Improved regime identification with validation\n",
        "def identify_market_regimes(embeddings, config):\n",
        "    \"\"\"\n",
        "    Cluster the embeddings to identify different market regimes with improved validation\n",
        "    \"\"\"\n",
        "    regime_labels = {}\n",
        "    regime_models = {}\n",
        "\n",
        "    for ticker, emb in embeddings.items():\n",
        "        print(f\"Identifying market regimes for {ticker}...\")\n",
        "\n",
        "        best_score = float('inf')\n",
        "        best_kmeans = None\n",
        "\n",
        "        for seed in range(5):\n",
        "            kmeans = KMeans(\n",
        "                n_clusters=config.N_REGIMES,\n",
        "                random_state=seed,\n",
        "                n_init=10,\n",
        "                max_iter=500\n",
        "            )\n",
        "            kmeans.fit(emb)\n",
        "\n",
        "            score = kmeans.inertia_\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_kmeans = kmeans\n",
        "\n",
        "        labels = best_kmeans.labels_\n",
        "\n",
        "        centers = best_kmeans.cluster_centers_\n",
        "        center_values = np.sum(centers, axis=1)\n",
        "        regime_order = np.argsort(center_values)\n",
        "\n",
        "        label_mapping = {regime_order[i]: i for i in range(config.N_REGIMES)}\n",
        "\n",
        "        ordered_labels = np.array([label_mapping[label] for label in labels])\n",
        "\n",
        "        regime_labels[ticker] = ordered_labels\n",
        "        regime_models[ticker] = best_kmeans\n",
        "\n",
        "        print(f\"Identified {config.N_REGIMES} regimes for {ticker}\")\n",
        "\n",
        "    return regime_labels, regime_models\n",
        "\n",
        "# 7. Enhanced visualization with better colors and clarity\n",
        "def visualize_regimes(embeddings, regime_labels, tickers, config):\n",
        "    \"\"\"\n",
        "    Visualize the identified market regimes with improved styling\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    regime_colors = config.REGIME_COLORS\n",
        "    regime_names = config.REGIME_NAMES\n",
        "\n",
        "    for i, ticker in enumerate(tickers):\n",
        "        if ticker not in embeddings or ticker not in regime_labels:\n",
        "            continue\n",
        "\n",
        "        print(f\"Running t-SNE for {ticker}...\")\n",
        "\n",
        "        max_samples = 5000\n",
        "        if len(embeddings[ticker]) > max_samples:\n",
        "            indices = np.random.choice(len(embeddings[ticker]), max_samples, replace=False)\n",
        "            emb_sample = embeddings[ticker][indices]\n",
        "            labels_sample = regime_labels[ticker][indices]\n",
        "        else:\n",
        "            emb_sample = embeddings[ticker]\n",
        "            labels_sample = regime_labels[ticker]\n",
        "\n",
        "        tsne = TSNE(\n",
        "            n_components=2,\n",
        "            method='barnes_hut',\n",
        "            angle=0.5,\n",
        "            perplexity=min(30, len(emb_sample) - 1),\n",
        "            n_iter=1000,\n",
        "            random_state=42,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        reduced_emb = tsne.fit_transform(emb_sample)\n",
        "\n",
        "        ax = fig.add_subplot(2, 2, i+1)\n",
        "\n",
        "        for regime in range(config.N_REGIMES):\n",
        "            mask = labels_sample == regime\n",
        "            ax.scatter(\n",
        "                reduced_emb[mask, 0],\n",
        "                reduced_emb[mask, 1],\n",
        "                c=regime_colors[regime],\n",
        "                label=regime_names[regime],\n",
        "                alpha=0.7,\n",
        "                edgecolor='w',\n",
        "                linewidth=0.5,\n",
        "                s=50\n",
        "            )\n",
        "\n",
        "        ax.set_title(f\"Market Regimes for {ticker}\", fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
        "        ax.set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
        "\n",
        "        ax.legend(title=\"Regimes\", fontsize=12)\n",
        "\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"market_regimes_tsne.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"t-SNE visualization saved to 'market_regimes_tsne.png'\")\n",
        "\n",
        "# 8. Improved time series visualization with regime mapping\n",
        "def map_regimes_to_timeseries(file_path, regime_labels, config):\n",
        "    \"\"\"\n",
        "    Map the identified regimes back to the original time series with improved visualization\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    has_date_column = 'date' in df.columns\n",
        "    if has_date_column:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df.sort_values('date', inplace=True)\n",
        "    else:\n",
        "        print(\"No 'date' column found. Creating a numerical index instead.\")\n",
        "        df['date'] = pd.RangeIndex(start=0, stop=len(df), step=1)\n",
        "\n",
        "    tickers = df['ticker'].unique() if 'ticker' in df.columns else ['data']\n",
        "\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "    for ticker in tickers:\n",
        "        if ticker not in regime_labels:\n",
        "            continue\n",
        "\n",
        "        print(f\"Mapping regimes to time series for {ticker}...\")\n",
        "\n",
        "        if 'ticker' in df.columns:\n",
        "            ticker_data = df[df['ticker'] == ticker].copy().reset_index(drop=True)\n",
        "        else:\n",
        "            ticker_data = df.copy().reset_index(drop=True)\n",
        "\n",
        "        price_col = 'close' if 'close' in ticker_data.columns else ticker_data.select_dtypes(include=[np.number]).columns[0]\n",
        "\n",
        "        regimes = np.full(len(ticker_data), np.nan)\n",
        "        window_size = config.WINDOW_SIZE\n",
        "\n",
        "        if len(regime_labels[ticker]) <= len(ticker_data) - window_size + 1:\n",
        "            regimes[window_size-1:window_size-1+len(regime_labels[ticker])] = regime_labels[ticker]\n",
        "        else:\n",
        "            regimes[window_size-1:] = regime_labels[ticker][:len(ticker_data)-(window_size-1)]\n",
        "\n",
        "        ticker_data['regime'] = regimes\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 16), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "        ax1.plot(ticker_data['date'], ticker_data[price_col], color='black', linewidth=2)\n",
        "        ax1.set_title(f\"{ticker} Price with Market Regimes\", fontsize=16, fontweight='bold')\n",
        "        ax1.set_ylabel(f\"{price_col} Price\", fontsize=14)\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        last_regime = None\n",
        "        start_idx = None\n",
        "\n",
        "        def regime_seen_before(regime_val, start_idx):\n",
        "            if pd.isna(regime_val) or start_idx is None or start_idx <= 0:\n",
        "                return False\n",
        "\n",
        "            prev_regimes = [\n",
        "                int(r) for r in ticker_data['regime'].iloc[:start_idx]\n",
        "                if pd.notna(r)\n",
        "            ]\n",
        "\n",
        "            return int(regime_val) in prev_regimes\n",
        "\n",
        "        for i, regime in enumerate(ticker_data['regime']):\n",
        "            if pd.notna(regime) and regime != last_regime:\n",
        "                if start_idx is not None and pd.notna(last_regime):\n",
        "                    regime_label = \"\"\n",
        "                    if not regime_seen_before(last_regime, start_idx):\n",
        "                        regime_label = config.REGIME_NAMES[int(last_regime)]\n",
        "\n",
        "                    ax1.axvspan(\n",
        "                        ticker_data['date'].iloc[start_idx],\n",
        "                        ticker_data['date'].iloc[i],\n",
        "                        alpha=0.2,\n",
        "                        color=config.REGIME_COLORS[int(last_regime)],\n",
        "                        label=regime_label\n",
        "                    )\n",
        "                start_idx = i\n",
        "                last_regime = regime\n",
        "\n",
        "        if start_idx is not None and pd.notna(last_regime):\n",
        "            regime_label = \"\"\n",
        "            if not regime_seen_before(last_regime, start_idx):\n",
        "                regime_label = config.REGIME_NAMES[int(last_regime)]\n",
        "\n",
        "            ax1.axvspan(\n",
        "                ticker_data['date'].iloc[start_idx],\n",
        "                ticker_data['date'].iloc[-1],\n",
        "                alpha=0.2,\n",
        "                color=config.REGIME_COLORS[int(last_regime)],\n",
        "                label=regime_label\n",
        "            )\n",
        "\n",
        "        handles, labels = ax1.get_legend_handles_labels()\n",
        "        by_label = dict(zip(labels, handles))\n",
        "        if \"\" in by_label:\n",
        "            del by_label[\"\"]\n",
        "        ax1.legend(by_label.values(), by_label.keys(), loc='upper left', fontsize=12)\n",
        "\n",
        "        for regime in range(config.N_REGIMES):\n",
        "            mask = ticker_data['regime'] == regime\n",
        "            if mask.any():\n",
        "                ax2.scatter(\n",
        "                    ticker_data.loc[mask, 'date'],\n",
        "                    np.ones(mask.sum()) * regime,\n",
        "                    color=config.REGIME_COLORS[regime],\n",
        "                    s=100,\n",
        "                    label=config.REGIME_NAMES[regime],\n",
        "                    alpha=0.7\n",
        "                )\n",
        "\n",
        "        ax2.set_yticks(range(config.N_REGIMES))\n",
        "        ax2.set_yticklabels(config.REGIME_NAMES)\n",
        "        ax2.set_ylabel(\"Market Regime\", fontsize=14)\n",
        "        ax2.set_xlabel(\"Date\" if has_date_column else \"Index\", fontsize=14)\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        if has_date_column:\n",
        "            fig.autofmt_xdate()\n",
        "\n",
        "        regime_descriptions = [\n",
        "            \"Bearish: Falling prices, negative momentum\",\n",
        "            \"Sideways: Range-bound, low volatility\",\n",
        "            \"Bullish: Rising prices, positive momentum\"\n",
        "        ]\n",
        "\n",
        "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
        "        ax1.text(\n",
        "            0.02, 0.02,\n",
        "            '\\n'.join(regime_descriptions),\n",
        "            transform=ax1.transAxes,\n",
        "            fontsize=12,\n",
        "            verticalalignment='bottom',\n",
        "            bbox=props\n",
        "        )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{ticker}_regimes_timeseries.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"Time series visualization saved to '{ticker}_regimes_timeseries.png'\")\n",
        "\n",
        "# 9. Main execution function with memory management\n",
        "def run_ts_tcc_regime_detection(file_path, config):\n",
        "    \"\"\"\n",
        "    Run the full TS-TCC pipeline for market regime detection with improved memory management\n",
        "    \"\"\"\n",
        "    print(\"Running TS-TCC Market Regime Detection with the following configuration:\")\n",
        "    for key, value in vars(config).items():\n",
        "        if key != 'DEVICE':\n",
        "            print(f\"{key}: {value}\")\n",
        "    print(f\"Using device: {config.DEVICE}\")\n",
        "\n",
        "    print(\"\\n=== Loading and Preprocessing Data ===\")\n",
        "    data, tickers = load_and_preprocess_data(file_path, config.WINDOW_SIZE)\n",
        "\n",
        "    print(\"\\n=== Training TS-TCC Models ===\")\n",
        "    models, embeddings = train_ts_tcc(data, tickers, config)\n",
        "\n",
        "    del data\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n=== Identifying Market Regimes ===\")\n",
        "    regime_labels, regime_models = identify_market_regimes(embeddings, config)\n",
        "\n",
        "    print(\"\\n=== Visualizing Market Regimes ===\")\n",
        "    visualize_regimes(embeddings, regime_labels, tickers, config)\n",
        "\n",
        "    print(\"\\n=== Mapping Regimes to Time Series ===\")\n",
        "    map_regimes_to_timeseries(file_path, regime_labels, config)\n",
        "\n",
        "    print(\"\\n=== Analysis Complete! ===\")\n",
        "    return models, embeddings, regime_labels, regime_models\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    FILE_PATH = \"melted_data.csv\"\n",
        "\n",
        "    config = Config()\n",
        "\n",
        "    models, embeddings, regime_labels, regime_models = run_ts_tcc_regime_detection(\n",
        "        file_path=FILE_PATH,\n",
        "        config=config\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFOI6AziH1DR",
        "outputId": "51814da2-396a-4479-c7f9-0df9ed60c79f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running TS-TCC Market Regime Detection with the following configuration:\n",
            "Using device: cuda\n",
            "\n",
            "=== Loading and Preprocessing Data ===\n",
            "Loading data...\n",
            "Data loaded: 40495 rows, 8 columns\n",
            "Found 1 unique tickers\n",
            "Processing ticker: data\n",
            "Created 40436 sequences of shape (60, 6)\n",
            "\n",
            "=== Training TS-TCC Models ===\n",
            "\n",
            "Training model for data with input dimension 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training data: 100%|██████████| 100/100 [42:48<00:00, 25.69s/it, loss=0.0406, lr=0.000000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting embeddings: 100%|██████████| 1264/1264 [00:02<00:00, 595.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 40436 embeddings for data\n",
            "\n",
            "=== Identifying Market Regimes ===\n",
            "Identifying market regimes for data...\n",
            "Identified 3 regimes for data\n",
            "\n",
            "=== Visualizing Market Regimes ===\n",
            "Running t-SNE for data...\n",
            "[t-SNE] Computing 91 nearest neighbors...\n",
            "[t-SNE] Indexed 5000 samples in 0.000s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/manifold/_t_sne.py:1164: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[t-SNE] Computed neighbors for 5000 samples in 0.317s...\n",
            "[t-SNE] Computed conditional probabilities for sample 1000 / 5000\n",
            "[t-SNE] Computed conditional probabilities for sample 2000 / 5000\n",
            "[t-SNE] Computed conditional probabilities for sample 3000 / 5000\n",
            "[t-SNE] Computed conditional probabilities for sample 4000 / 5000\n",
            "[t-SNE] Computed conditional probabilities for sample 5000 / 5000\n",
            "[t-SNE] Mean sigma: 2.585286\n",
            "[t-SNE] KL divergence after 250 iterations with early exaggeration: 81.909134\n",
            "[t-SNE] KL divergence after 1000 iterations: 1.188588\n",
            "t-SNE visualization saved to 'market_regimes_tsne.png'\n",
            "\n",
            "=== Mapping Regimes to Time Series ===\n",
            "No 'date' column found. Creating a numerical index instead.\n",
            "Mapping regimes to time series for data...\n",
            "Time series visualization saved to 'data_regimes_timeseries.png'\n",
            "\n",
            "=== Analysis Complete! ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DqgZTPY7SCve"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}